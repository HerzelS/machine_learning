---
title: "Naive Bayes"
author: "Mukandi Herzel Shingirirai"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base")
```


## Machine Learning


### Import Packages

```{python}

# Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
# Generate a synthetic dataset for classification
from sklearn.datasets import make_classification
import warnings
warnings.filterwarnings('ignore')
```



### Load and Explore the dataset

```{python}
# Create a list of columns to add them as column names, since the dataset does not have column names
cols = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
df = pd.read_csv('data/magic04.data', names=cols)
df.head()
```


```{python}
# Check the class
df["class"].unique()
```

```{python}
# Lets convert "class" to numeric
df["class"] = (df["class"] == "g").astype(int)
```



```{python}
# Now let us recheck "class"
df["class"].unique()
```



```{python}
df.head()
```



```{python}
for label in cols[:-1]:
  plt.hist(df[df["class"] == 1][label], color="blue", label="gamma", alpha=0.7, density=True)
  plt.hist(df[df["class"] == 0][label], color="red", label="hedron", alpha=0.7, density=True)
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()
```



## Preparing the Train, Validation and Test Datasets

```{python}
train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
```



```{python}
def scale_dataset(dataframe, oversample=False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)

  if oversample:
    x, y = RandomOverSampler().fit_resample(x, y)

  # Create the whole data as a horizontal numpy array
  data = np.hstack((x, np.reshape(y, (-1, 1))))

  return data, x, y
```




```{python}
print(len(train[train["class"] == 1])) # Gamma
print(len(train[train["class"] == 0])) # Hadron
```


We want to oversample our data so that these match.

```{python}
train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)
```


```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
```


```{python}
knn_model = KNeighborsClassifier(n_neighbors = 5)
knn_model.fit(x_train, y_train)
```

```{python}
y_pred = knn_model.predict(x_test)
```


```{python}
print(classification_report(y_test, y_pred))
```




* Accuracy is 80%
* Prescision for hadron is 76% and gamma is 82%

Precision:

<div align="center">
$$
\text{Prescision} = \frac{\text{True positives}}{\text{True Positives} +
\text{False Positives}}
$$
</div>


Recall


<div align="center">

$$
\text{Recall} = \frac{\text{True Postives}}{\text{False Negatives} +
\text{True Positives}}
$$

</div>


## Naive Bayes Model

#### What is Naive Bayes?
Naive Bayes is a **probabilistic** machine learning algorithm based on **Bayes' Theorem**, and it is particularly well-suited for **classification problems**. It assumes that the features used for classification are **independent**, hence the term "naive."

Despite this assumption often being unrealistic in real-world data (as features are usually correlated), Naive Bayes performs surprisingly well in many cases.

#### How does Naive Bayes work?
Naive Bayes classifies data by calculating the **posterior probability** of each class and selecting the class with the highest probability.

The formula used is based on **Bayes' Theorem**:

$$
P(C \mid X) = \frac{P(X \mid C) \cdot P(C)}{P(X)}
$$

Where:
- \( P(C \mid X) \) is the **posterior probability** of class \( C \) given the features \( X \).
- \( P(X \mid C) \) is the **likelihood** of observing the features \( X \) given class \( C \).
- \( P(C) \) is the **prior probability** of class \( C \).
- \( P(X) \) is the **evidence** or probability of the features \( X \).

The "naive" part comes from the assumption that the features \( X = (x_1, x_2, \ldots, x_n) \) are **independent**, so we can calculate the likelihood as:

$$
P(X \mid C) = P(x_1 \mid C) \cdot P(x_2 \mid C) \cdot \ldots \cdot P(x_n \mid C)
$$

This simplifies the model and makes it computationally feasible.

#### Steps for Naive Bayes:
1. Calculate the **prior probability** for each class \( P(C) \).
2. Calculate the **likelihood** of each feature given the class \( P(x_i \mid C) \).
3. Apply **Bayes' Theorem** to compute the **posterior probability** for each class.
4. Select the class with the highest **posterior probability**.



**Bayes Rule**

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$


Example

<div align="center">

Covid Test Result


|            | Status | Positive | Negative |Total  |
|------------|--------|----------|----------|-------|
|            | Y      | 531      | 6        | 537   |
|  Has Covid?| N      | 20       | 9443     | 9463  |
|            | Total  | 551      | 9449     | 10000 |

</div>


Categories:


1.   People who have Covid and test positive
2.   People who have COVID but test negative
3.   People who do not have COVID and test negative
4.   People who do not have COVID but test positive


Question: What is the probabilioty of having COVID given a positive test?


<div align="center">

$$
\text{P(COVID | + test)} = \frac{\text{531}}{\text{551}}
= 96.4%
$$

</div>


**Bayes' Rule**

- This rule asks, what is the probability of some event A happening, given that B happened.

<div align="center">

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

</div>

In the equaiton:

* B is the condition

Now the question is that what if we don't have data on the condition. We can work using the formula.

Application

Lets say we have some disease and we know that:

* P(false positive) = 0.05
* P(false negative) = 0.01
* P(disease) = 0.1
* P(disease | (+)test) = ?


</div>

|            |Positive  | Negative |Total |
|------------|----------|----------|------|
|  Disease   | 0.99     | 0.01     | 1    |
|  No disease| 0.05     | 0.95     | 1    |

</div>



$$
P({disease} \mid +) = \frac{P(+ \mid \text{disease}) \cdot P({disease})}{P(+)} = \frac{0.99 \cdot 0.1}{P(+)} = \frac{0.99 \cdot 0.1}{P(+ \mid disease) \cdot P(disease) + P(+ \mid no-disease) \cdot P(no-disease)} = \frac{0.99 \cdot 0.1}{099 \cdot 0.1 + 0.05 \cdot 0.9} = 0.6875 \space or \space 68.75 \space percent
$$

We can expand this equation and this is what is called **Naive Bayes**

Terminology:

* Posterior = P(C<sub>k</sub> | x) - what is the probability of some class CK (categories or classes) e.g. C<sub>1</sub> might be Cats, C<sub>2</sub> might be Dogs and C<sub>3</sub> might be Parrots....C<sub>k</sub>
* x is the feature vector - so the expression *P(C<sub>k</sub> | x)* means, what is the probability that it is actually from this class given all this evidence that we see in all the x's.
* likelihood = P(x | C<sub>k</sub>) - what is the likelihood of actually seeing x given all these different features from that category.
* prior = P(C<sub>k</sub>) - in the entire population of things, what is the probability in general? e.g. In my entire dataset, what is the probability that this image is a cat?
* evidence = P(x) - probability of x


Rule for Naive Bayes:

$$
P(C_k \mid x_1, x_2, x_3, \ldots, x_n) \propto P(C_k) \cdot \pi^n_{i = 1} P(x_i \mid C_k)
$$


What is this **P(C<sub>k</sub> | x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>n</sub>)** side of the equation asking? It is asking what is the probability that we are in some class K, given all the x inputs from x<sub>1</sub> to x<sub>n</sub>. For example if we are predicting whether we play soccer today give whether its raining, day of the  week, wind speed, time of the day.

Derivation

$$
P(C_k \mid x_1, x_2, x_3, \ldots, x_n) = \frac{P(x_1, x_2, x_3, \ldots, x_n \mid C_k) ⋅ P(C_k)}{P(x_1, x_2, x_3, \ldots, x_n)}
$$

NB: You will notice that the denominator has no impact on the class so C<sub>k</sub> is going to be constant for all our different classes. So we can say:


$$
{P{C_k \mid x_1, x_2, x_3, \ldots, x_n}} = {P(x_1, x_2, x_3, \ldots, x_n \mid C_k) ⋅ P(C_k)}
$$


Naive Bayes assumes that our "x"s are independent. So we can just expand as:

$$
{P(x_1, x_2, x_3, \ldots, x_n \mid C_k)} = {P{(x_1 \mid C_k) ⋅ P(x_1 \mid C_k)... ⋅ P(x_n \mid C_k)}}
$$


We can also expand P(C<sub>k</sub> | x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>n</sub>) as:

$$
P(C_k) \prod_{i=1}^n P(x_i \mid C_k)
$$

Summing up: This is saying the probabiulut that we are in some category given that we have all these different features is proportional to the probability of that class in general times the probability of those features given that we are in this one class that we are testing. E.g. Probability that we will play soccer today given that its raining, not windy and it Wednesday is propotional to "what is the probability that we play soccer anyways" times the probablity that "its rainy given that we play soccer" times the propability that "its windy given that we are playing soccer" i.e. how many times are we playing soccer when its not windy and what is the probability that its Wednesday and we are playing soccer.

How do we then use this to make a classification:

The predicted value is denoted by \\( \hat{y} \\), which is an estimate of the true value \\( y \\).


$$
\hat{y} = argmax_{k \sum(1,..k)}P(C_k) \prod_{i=1}^n P(x_i \mid C_k)
$$

This approach is known as Maximum A Posteriori.

# Now back to the code

## Naive Bayes

```{python}
from sklearn.naive_bayes import GaussianNB
```


```{python}
nb_model = GaussianNB()
nb_model = nb_model.fit(x_train, y_train)
```



```{python}
y_pred = nb_model.predict(x_test)
print(classification_report(y_test, y_pred))
```


