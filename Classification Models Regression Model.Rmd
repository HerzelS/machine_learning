---
title: "Regression Model"
author: "Mukandi Herzel Shingirirai"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base")
```


### Logistic Regression

### The basis for the Sigmoid function notation

Sometimes we write the sigmoid more generally as:

$$
S(y) = \frac{1}{1 + e^{-y}}
$$

Where \( y = mx + b \). This gives us the classic **S-shaped curve** of logistic regression.

<br>

#### How can we model probability?

<br>


$$
p = mx + b
$$

<br>

However, the problem is that x can be from infnity to infinity while one of the rules for probability is that it has to stay between zero and 1.

So instead of setting the probability, we can set the odds equal to this and it becomes a ratio:

<br>

$$
\frac{p}{1 - p} = mx + b
$$

<br>

This ratio is allowed to take on infinity values. However, still *mx + b* can still be negative. We fix that by takign the log of the odds:

<br>

$$
ln(\frac{p}{1 - p}) = mx + b
$$

<br>

How then do we solve for p - probability.

<br>

First thing is you can remove the log by taking the e:

<br>

$$
_{e}ln(\frac{p}{1 - p}) = _{e}mx + b
$$

<br>

thus:

<br>

$$
\frac{P}{1-P} = _{e}mx + b
$$

<br>

so:

<br>

$$
P = (1-P)_{e}mx + b = {e}^{mx + b} - pe^{mx + b}
$$
<br>

thus:

<br>

$$
P(1 + e^{mx + b}) = e^{mx + b}
$$
<br>

and:

<br>

$$
p = \frac{e^{mx + b}}{1 + e^1}
$$

<br>

To get a numerator of 1 on top:

<br>

$$
p = \frac{e^{mx + b}}{1 + e^1} ⋅ \frac{e^{(-mx + b)}}{e^{(-mx + b)}}
$$
<br>

Probability then becomes:

<br>

$$
p = \frac{1}{1 + e^{(-mx + b)}}
$$

<br>

This isa special function called the sigmoid function which looks like:

<br>

$$
S(y) = \frac{1}{1 + e^{(-y)}}
$$


<br>
The sigmoid function gives us the curved s that we find in logistic regression.

<br>

---


### Real-Life Examples of Logistic Regression

<br>

1. **Email Spam Classification**  
   - Input: Frequency of spammy words, sender reputation  
   - Output: Probability that an email is spam (1 = spam, 0 = not spam)

<br>

2. **Loan Default Prediction**  
   - Input: Credit score, income, loan amount  
   - Output: Probability a person will default on their loan

<br>

3. **Medical Diagnosis**  
   - Input: Age, blood pressure, test results  
   - Output: Probability a patient has a disease (e.g., heart disease)

<br>

4. **Customer Churn**  
   - Input: Usage patterns, support tickets, subscription duration  
   - Output: Probability a customer will cancel the service

<br>

5. **Online Advertising**  
   - Input: User demographics, ad features, time of day  
   - Output: Probability that a user will click an ad

---

Logistic regression helps us answer **yes-or-no questions** based on input data — making it one of the most widely used classification tools in data science.

<br>

## Logistic Regression Model 

<br>

### Import Packages

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')
```

<br>

### Load and Explore the dataset

```{python}
# Create a list of columns to add them as column names, since the dataset does not have column names
cols = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
df = pd.read_csv('data/magic04.data', names=cols)
df.head()
```

<br>

```{python}
# Check the class
df["class"].unique()
```

<br>

```{python}
# Lets convert "class" to numeric
df["class"] = (df["class"] == "g").astype(int)
```

<br>

```{python}
# Now let us recheck "class"
df["class"].unique()
```

<br>

```{python}
df.head()
```

<br>

```{python}
for label in cols[:-1]:
  plt.hist(df[df["class"] == 1][label], color="blue", label="gamma", alpha=0.7, density=True)
  plt.hist(df[df["class"] == 0][label], color="red", label="hedron", alpha=0.7, density=True)
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()
```

<br>

### Example 1:  Preparing the Train, Validation and Test Datasets

```{python}
train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
```

<br>

```{python}
def scale_dataset(dataframe, oversample=False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)

  if oversample:
    x, y = RandomOverSampler().fit_resample(x, y)

  # Create the whole data as a horizontal numpy array
  data = np.hstack((x, np.reshape(y, (-1, 1))))

  return data, x, y
```

<br>

```{python}
print(len(train[train["class"] == 1])) # Gamma
print(len(train[train["class"] == 0])) # Hadron
```

<br>

We want to oversample our data so that these match.

```{python}
train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)
```

<br>

### Example 2:  Preparing the Train, Validation and Test Datasets

<br>

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Generate values for x (input)
x = np.linspace(-10, 10, 400)

# Apply the sigmoid function
y = sigmoid(x)

# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(x, y, label=r'$\frac{1}{1 + e^{-x}}$', color='b', lw=2)

# Add labels and title
plt.title("Sigmoid Function", fontsize=16)
plt.xlabel("x", fontsize=14)
plt.ylabel("Sigmoid(x)", fontsize=14)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.grid(True)

# Show the plot
plt.legend()
plt.show()
```

<br>

```{python}
lg_model = LogisticRegression()
lg_model = lg_model.fit(x_train, y_train)
```

<br>

```{python}
y_pred = lg_model.predict(x_test)
print(classification_report(y_test, y_pred))
```

















