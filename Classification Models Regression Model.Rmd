---
title: "Regression Model"
author: "Mukandi Herzel Shingirirai"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base")
```


## Machine Learning

### Import Packages

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
import warnings
warnings.filterwarnings('ignore')
```


### Load and Explore the dataset

```{python}
# Create a list of columns to add them as column names, since the dataset does not have column names
cols = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
df = pd.read_csv('data/magic04.data', names=cols)
df.head()
```


```{python}
# Check the class
df["class"].unique()
```

```{python}
# Lets convert "class" to numeric
df["class"] = (df["class"] == "g").astype(int)
```



```{python}
# Now let us recheck "class"
df["class"].unique()
```



```{python}
df.head()
```



```{python}
for label in cols[:-1]:
  plt.hist(df[df["class"] == 1][label], color="blue", label="gamma", alpha=0.7, density=True)
  plt.hist(df[df["class"] == 0][label], color="red", label="hedron", alpha=0.7, density=True)
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()
```



## Preparing the Train, Validation and Test Datasets

```{python}
train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
```



```{python}
def scale_dataset(dataframe, oversample=False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)

  if oversample:
    x, y = RandomOverSampler().fit_resample(x, y)

  # Create the whole data as a horizontal numpy array
  data = np.hstack((x, np.reshape(y, (-1, 1))))

  return data, x, y
```




```{python}
print(len(train[train["class"] == 1])) # Gamma
print(len(train[train["class"] == 0])) # Hadron
```


We want to oversample our data so that these match.

```{python}
train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)
```

### Logistic Regression

How can we model probability?

$$
p = mx + b
$$

However, the problem is that x can be from infnity to infinity while one of the rules for probability is that it has to stay between zero and 1.

So instead of setting the probability, we can set the odds equal to this and it becomes a ratio:

$$
\frac{p}{1 - p} = mx + b
$$

This ratio is allowed to take on infinity values. However, still *mx + b* can still be negative. We fix that by takign the log of the odds:

$$
ln(\frac{p}{1 - p}) = mx + b
$$

How then do we solve for p - probablity.

First thing is you can remove the log by taking the e:
$$
_{e}ln(\frac{p}{1 - p}) = _{e}mx + b
$$

thus:

$$
\frac{P}{1-P} = _{e}mx + b
$$

so:

$$
P = (1-P)_{e}mx + b = {e}^{mx + b} - pe^{mx + b}
$$

thus:


$$
P(1 + e^{mx + b}) = e^{mx + b}
$$

and:

$$
p = \frac{e^{mx + b}}{1 + e^1}
$$

To get a numerator of 1 on top:

$$
p = \frac{e^{mx + b}}{1 + e^1} â‹… \frac{e^{(-mx + b)}}{e^{(-mx + b)}}
$$

Probability then becomes:

$$
p = \frac{1}{1 + e^{(-mx + b)}}
$$

This isa special function called the sigmoid function which looks like:

$$
S(y) = \frac{1}{1 + e^{(-y)}}
$$

The sigmoid function gives us the curved s that we find in logistic regression.


### ðŸš€ Logistic Regression â€“ An Intuitive Introduction

#### How can we model probability?

Letâ€™s start with a simple linear equation:

$$
p = mx + b
$$

But thereâ€™s a problem:  
The left side, \( p \), represents a probability â€” and **probabilities must be between 0 and 1**.  
However, the right side, \( mx + b \), can take on **any value** from \( -\infty \) to \( +\infty \).  
This violates the basic rule of probabilities.

---

#### So what do we do?  
Instead of modeling the probability directly, we model the **odds** of the event happening:

$$
\text{Odds} = \frac{p}{1 - p}
$$

This ratio tells us how likely an event is to happen compared to it **not** happening.  
The odds can range from 0 to \( +\infty \), which is better â€” but we can do even better.

---

#### Letâ€™s take it further: the log-odds (logit)

We now apply the natural logarithm (ln) to the odds:

$$
\ln \left( \frac{p}{1 - p} \right) = mx + b
$$

This transforms the output to range from \( -\infty \) to \( +\infty \), which fits perfectly with a linear equation.  
This transformation is called the **logit** function â€” and this is what logistic regression models.

---

#### Solving for the probability \( p \)

Now, letâ€™s isolate \( p \). Starting from:

$$
\ln \left( \frac{p}{1 - p} \right) = mx + b
$$

We exponentiate both sides to cancel the log:

$$
\frac{p}{1 - p} = e^{mx + b}
$$

Now solve for \( p \):

$$
p = \frac{e^{mx + b}}{1 + e^{mx + b}}
$$

---

#### Simplifying to the sigmoid function

The final form is:

$$
p = \frac{1}{1 + e^{-(mx + b)}}
$$

This function is called the **sigmoid** (or logistic) function. It maps any real value (from \( -\infty \) to \( +\infty \)) smoothly into the interval \( (0, 1) \).  
Thatâ€™s perfect for modeling probabilities.

---

#### Sigmoid function notation

Sometimes we write the sigmoid more generally as:

$$
S(y) = \frac{1}{1 + e^{-y}}
$$

Where \( y = mx + b \). This gives us the classic **S-shaped curve** of logistic regression.

---

#### ðŸ“Œ Real-Life Examples of Logistic Regression

1. **Email Spam Classification**  
   - Input: Frequency of spammy words, sender reputation  
   - Output: Probability that an email is spam (1 = spam, 0 = not spam)

2. **Loan Default Prediction**  
   - Input: Credit score, income, loan amount  
   - Output: Probability a person will default on their loan

3. **Medical Diagnosis**  
   - Input: Age, blood pressure, test results  
   - Output: Probability a patient has a disease (e.g., heart disease)

4. **Customer Churn**  
   - Input: Usage patterns, support tickets, subscription duration  
   - Output: Probability a customer will cancel the service

5. **Online Advertising**  
   - Input: User demographics, ad features, time of day  
   - Output: Probability that a user will click an ad

---

Logistic regression helps us answer **yes-or-no questions** based on input data â€” making it one of the most widely used classification tools in data science.



```{python}
import numpy as np
import matplotlib.pyplot as plt

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Generate values for x (input)
x = np.linspace(-10, 10, 400)

# Apply the sigmoid function
y = sigmoid(x)

# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(x, y, label=r'$\frac{1}{1 + e^{-x}}$', color='b', lw=2)

# Add labels and title
plt.title("Sigmoid Function", fontsize=16)
plt.xlabel("x", fontsize=14)
plt.ylabel("Sigmoid(x)", fontsize=14)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.grid(True)

# Show the plot
plt.legend()
plt.show()
```


```{python}
from sklearn.linear_model import LogisticRegression
```


```{python}
lg_model = LogisticRegression()
lg_model = lg_model.fit(x_train, y_train)
```


```{python}
y_pred = lg_model.predict(x_test)
print(classification_report(y_test, y_pred))
```






















