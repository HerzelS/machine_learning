---
title: "SVM"
author: "Mukandi Herzel Shingirirai"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base")
```


## Machine Learning

### K Nearest Neighbours (kNN)



### Import Packages

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
import warnings
warnings.filterwarnings('ignore')
```


### Load and Explore the dataset

```{python}
# Create a list of columns to add them as column names, since the dataset does not have column names
cols = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
df = pd.read_csv('data/magic04.data', names=cols)
df.head()
```


```{python}
# Check the class
df["class"].unique()
```

```{python}
# Lets convert "class" to numeric
df["class"] = (df["class"] == "g").astype(int)
```



```{python}
# Now let us recheck "class"
df["class"].unique()
```



```{python}
df.head()
```



```{python}
for label in cols[:-1]:
  plt.hist(df[df["class"] == 1][label], color="blue", label="gamma", alpha=0.7, density=True)
  plt.hist(df[df["class"] == 0][label], color="red", label="hedron", alpha=0.7, density=True)
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()
```



## Preparing the Train, Validation and Test Datasets

```{python}
train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
```



```{python}
def scale_dataset(dataframe, oversample=False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)

  if oversample:
    x, y = RandomOverSampler().fit_resample(x, y)

  # Create the whole data as a horizontal numpy array
  data = np.hstack((x, np.reshape(y, (-1, 1))))

  return data, x, y
```




```{python}
print(len(train[train["class"] == 1])) # Gamma
print(len(train[train["class"] == 0])) # Hadron
```


We want to oversample our data so that these match.

```{python}
train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)
```


```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
```


```{python}
knn_model = KNeighborsClassifier(n_neighbors = 5)
knn_model.fit(x_train, y_train)
```

```{python}
y_pred = knn_model.predict(x_test)
```


```{python}
print(classification_report(y_test, y_pred))
```




* Accuracy is 80%
* Prescision for hadron is 76% and gamma is 82%

Precision:

<div align="center">
$$
\text{Prescision} = \frac{\text{True positives}}{\text{True Positives} +
\text{False Positives}}
$$
</div>


Recall


<div align="center">

$$
\text{Recall} = \frac{\text{True Postives}}{\text{False Negatives} +
\text{True Positives}}
$$

</div>


## Naive Bayes Model

#### What is Naive Bayes?
Naive Bayes is a **probabilistic** machine learning algorithm based on **Bayes' Theorem**, and it is particularly well-suited for **classification problems**. It assumes that the features used for classification are **independent**, hence the term "naive."

Despite this assumption often being unrealistic in real-world data (as features are usually correlated), Naive Bayes performs surprisingly well in many cases.

#### How does Naive Bayes work?
Naive Bayes classifies data by calculating the **posterior probability** of each class and selecting the class with the highest probability.

The formula used is based on **Bayes' Theorem**:

$$
P(C \mid X) = \frac{P(X \mid C) \cdot P(C)}{P(X)}
$$

Where:
- \( P(C \mid X) \) is the **posterior probability** of class \( C \) given the features \( X \).
- \( P(X \mid C) \) is the **likelihood** of observing the features \( X \) given class \( C \).
- \( P(C) \) is the **prior probability** of class \( C \).
- \( P(X) \) is the **evidence** or probability of the features \( X \).

The "naive" part comes from the assumption that the features \( X = (x_1, x_2, \ldots, x_n) \) are **independent**, so we can calculate the likelihood as:

$$
P(X \mid C) = P(x_1 \mid C) \cdot P(x_2 \mid C) \cdot \ldots \cdot P(x_n \mid C)
$$

This simplifies the model and makes it computationally feasible.

#### Steps for Naive Bayes:
1. Calculate the **prior probability** for each class \( P(C) \).
2. Calculate the **likelihood** of each feature given the class \( P(x_i \mid C) \).
3. Apply **Bayes' Theorem** to compute the **posterior probability** for each class.
4. Select the class with the highest **posterior probability**.



**Bayes Rule**

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$


Example

<div align="center">

Covid Test Result


|            | Status | Positive | Negative |Total  |
|------------|--------|----------|----------|-------|
|            | Y      | 531      | 6        | 537   |
|  Has Covid?| N      | 20       | 9443     | 9463  |
|            | Total  | 551      | 9449     | 10000 |

</div>


Categories:


1.   People who have Covid and test positive
2.   People who have COVID but test negative
3.   People who do not have COVID and test negative
4.   People who do not have COVID but test positive


Question: What is the probabilioty of having COVID given a positive test?


<div align="center">

$$
\text{P(COVID | + test)} = \frac{\text{531}}{\text{551}}
= 96.4%
$$

</div>


**Bayes' Rule**

- This rule asks, what is the probability of some event A happening, given that B happened.

<div align="center">

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

</div>

In the equaiton:

* B is the condition

Now the question is that what if we don't have data on the condition. We can work using the formula.

Application

Lets say we have some disease and we know that:

* P(false positive) = 0.05
* P(false negative) = 0.01
* P(disease) = 0.1
* P(disease | (+)test) = ?


</div>

|            |Positive  | Negative |Total |
|------------|----------|----------|------|
|  Disease   | 0.99     | 0.01     | 1    |
|  No disease| 0.05     | 0.95     | 1    |

</div>



$$
P({disease} \mid +) = \frac{P(+ \mid \text{disease}) \cdot P({disease})}{P(+)} = \frac{0.99 \cdot 0.1}{P(+)} = \frac{0.99 \cdot 0.1}{P(+ \mid disease) \cdot P(disease) + P(+ \mid no-disease) \cdot P(no-disease)} = \frac{0.99 \cdot 0.1}{099 \cdot 0.1 + 0.05 \cdot 0.9} = 0.6875 \space or \space 68.75 \space percent
$$

We can expand this equation and this is what is called **Naive Bayes**

Terminology:

* Posterior = P(C<sub>k</sub> | x) - what is the probability of some class CK (categories or classes) e.g. C<sub>1</sub> might be Cats, C<sub>2</sub> might be Dogs and C<sub>3</sub> might be Parrots....C<sub>k</sub>
* x is the feature vector - so the expression *P(C<sub>k</sub> | x)* means, what is the probability that it is actually from this class given all this evidence that we see in all the x's.
* likelihood = P(x | C<sub>k</sub>) - what is the likelihood of actually seeing x given all these different features from that category.
* prior = P(C<sub>k</sub>) - in the entire population of things, what is the probability in general? e.g. In my entire dataset, what is the probability that this image is a cat?
* evidence = P(x) - probability of x


Rule for Naive Bayes:

$$
P(C_k \mid x_1, x_2, x_3, \ldots, x_n) \propto P(C_k) \cdot \pi^n_{i = 1} P(x_i \mid C_k)
$$


What is this **P(C<sub>k</sub> | x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>n</sub>)** side of the equation asking? It is asking what is the probability that we are in some class K, given all the x inputs from x<sub>1</sub> to x<sub>n</sub>. For example if we are predicting whether we play soccer today give whether its raining, day of the  week, wind speed, time of the day.

Derivation

$$
P(C_k \mid x_1, x_2, x_3, \ldots, x_n) = \frac{P(x_1, x_2, x_3, \ldots, x_n \mid C_k) â‹… P(C_k)}{P(x_1, x_2, x_3, \ldots, x_n)}
$$

NB: You will notice that the denominator has no impact on the class so C<sub>k</sub> is going to be constant for all our different classes. So we can say:


$$
{P{C_k \mid x_1, x_2, x_3, \ldots, x_n}} = {P(x_1, x_2, x_3, \ldots, x_n \mid C_k) â‹… P(C_k)}
$$


Naive Bayes assumes that our "x"s are independent. So we can just expand as:

$$
{P(x_1, x_2, x_3, \ldots, x_n \mid C_k)} = {P{(x_1 \mid C_k) â‹… P(x_1 \mid C_k)... â‹… P(x_n \mid C_k)}}
$$


We can also expand P(C<sub>k</sub> | x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>n</sub>) as:

$$
P(C_k) \prod_{i=1}^n P(x_i \mid C_k)
$$

Summing up: This is saying the probabiulut that we are in some category given that we have all these different features is proportional to the probability of that class in general times the probability of those features given that we are in this one class that we are testing. E.g. Probability that we will play soccer today given that its raining, not windy and it Wednesday is propotional to "what is the probability that we play soccer anyways" times the probablity that "its rainy given that we play soccer" times the propability that "its windy given that we are playing soccer" i.e. how many times are we playing soccer when its not windy and what is the probability that its Wednesday and we are playing soccer.

How do we then use this to make a classification:

The predicted value is denoted by \\( \hat{y} \\), which is an estimate of the true value \\( y \\).


$$
\hat{y} = argmax_{k \sum(1,..k)}P(C_k) \prod_{i=1}^n P(x_i \mid C_k)
$$

This approach is known as Maximum A Posteriori.

# Now back to the code

## Naive Bayes

```{python}
from sklearn.naive_bayes import GaussianNB
```


```{python}
nb_model = GaussianNB()
nb_model = nb_model.fit(x_train, y_train)
```



```{python}
y_pred = nb_model.predict(x_test)
print(classification_report(y_test, y_pred))
```



### Logistic Regression

How can we model probability?

$$
p = mx + b
$$

However, the problem is that x can be from infnity to infinity while one of the rules for probability is that it has to stay between zero and 1.

So instead of setting the probability, we can set the odds equal to this and it becomes a ratio:

$$
\frac{p}{1 - p} = mx + b
$$

This ratio is allowed to take on infinity values. However, still *mx + b* can still be negative. We fix that by takign the log of the odds:

$$
ln(\frac{p}{1 - p}) = mx + b
$$

How then do we solve for p - probablity.

First thing is you can remove the log by taking the e:
$$
_{e}ln(\frac{p}{1 - p}) = _{e}mx + b
$$

thus:

$$
\frac{P}{1-P} = _{e}mx + b
$$

so:

$$
P = (1-P)_{e}mx + b = {e}^{mx + b} - pe^{mx + b}
$$

thus:


$$
P(1 + e^{mx + b}) = e^{mx + b}
$$

and:

$$
p = \frac{e^{mx + b}}{1 + e^1}
$$

To get a numerator of 1 on top:

$$
p = \frac{e^{mx + b}}{1 + e^1} â‹… \frac{e^{(-mx + b)}}{e^{(-mx + b)}}
$$

Probability then becomes:

$$
p = \frac{1}{1 + e^{(-mx + b)}}
$$

This isa special function called the sigmoid function which looks like:

$$
S(y) = \frac{1}{1 + e^{(-y)}}
$$

The sigmoid function gives us the curved s that we find in logistic regression.



```{python}
# Naive Bayes Explanation and Code

# Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Generate a synthetic dataset for classification
from sklearn.datasets import make_classification

# Naive Bayes: An Intuitive Introduction
"""
What is Naive Bayes?
Naive Bayes is a probabilistic machine learning algorithm based on Bayes' Theorem, and it is particularly well-suited for classification problems. It assumes that the features used for classification are independent, hence the term "naive."
Despite this assumption often being unrealistic in real-world data (as features are usually correlated), Naive Bayes performs surprisingly well in many cases.

How does Naive Bayes work?
Naive Bayes classifies data by calculating the posterior probability of each class and selecting the class with the highest probability.
The formula used is based on Bayes' Theorem:

    P(C | X) = (P(X | C) * P(C)) / P(X)

Where:
- P(C | X) is the posterior probability of class C given the features X.
- P(X | C) is the likelihood of observing the features X given class C.
- P(C) is the prior probability of class C.
- P(X) is the evidence or probability of the features X.

The "naive" part comes from the assumption that the features X = (x1, x2, ..., xn) are independent, so we can calculate the likelihood as:

    P(X | C) = P(x1 | C) * P(x2 | C) * ... * P(xn | C)

This simplifies the model and makes it computationally feasible.
"""

# Create a simple 2D dataset for classification
X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_classes=2, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the Naive Bayes classifier (Gaussian Naive Bayes)
nb = GaussianNB()
nb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nb.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Naive Bayes: {accuracy * 100:.2f}%")

# Visualize the dataset and the decision boundary of Naive Bayes
plt.figure(figsize=(8, 6))

# Plot the decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = nb.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=100, edgecolors='k', cmap=plt.cm.coolwarm, label="Train Data")
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=100, edgecolors='k', marker='X', cmap=plt.cm.coolwarm, label="Test Data")

plt.title("Naive Bayes Classification", fontsize=16)
plt.xlabel("Feature 1", fontsize=14)
plt.ylabel("Feature 2", fontsize=14)
plt.legend(loc="upper left")
plt.show()

```



Explanation:
Naive Bayes: Iâ€™ve included a brief explanation of Naive Bayes theory at the start of the notebook. You can easily see how the classifier works based on Bayes' Theorem.

Code Implementation:

We generate a synthetic 2D dataset using make_classification and split it into a training and testing set.

We use the Gaussian Naive Bayes classifier from sklearn (GaussianNB) for continuous data.

After training the classifier on the training set (X_train, y_train), predictions are made on the test set (X_test), and accuracy is computed.

Visualization:

The code generates a plot showing the decision boundary learned by the Naive Bayes model, which classifies points into two classes.

The train data is shown with large circles, and the test data is shown with smaller X-marked points.

### ðŸš€ Logistic Regression â€“ An Intuitive Introduction

#### How can we model probability?

Letâ€™s start with a simple linear equation:

$$
p = mx + b
$$

But thereâ€™s a problem:  
The left side, \( p \), represents a probability â€” and **probabilities must be between 0 and 1**.  
However, the right side, \( mx + b \), can take on **any value** from \( -\infty \) to \( +\infty \).  
This violates the basic rule of probabilities.

---

#### So what do we do?  
Instead of modeling the probability directly, we model the **odds** of the event happening:

$$
\text{Odds} = \frac{p}{1 - p}
$$

This ratio tells us how likely an event is to happen compared to it **not** happening.  
The odds can range from 0 to \( +\infty \), which is better â€” but we can do even better.

---

#### Letâ€™s take it further: the log-odds (logit)

We now apply the natural logarithm (ln) to the odds:

$$
\ln \left( \frac{p}{1 - p} \right) = mx + b
$$

This transforms the output to range from \( -\infty \) to \( +\infty \), which fits perfectly with a linear equation.  
This transformation is called the **logit** function â€” and this is what logistic regression models.

---

#### Solving for the probability \( p \)

Now, letâ€™s isolate \( p \). Starting from:

$$
\ln \left( \frac{p}{1 - p} \right) = mx + b
$$

We exponentiate both sides to cancel the log:

$$
\frac{p}{1 - p} = e^{mx + b}
$$

Now solve for \( p \):

$$
p = \frac{e^{mx + b}}{1 + e^{mx + b}}
$$

---

#### Simplifying to the sigmoid function

The final form is:

$$
p = \frac{1}{1 + e^{-(mx + b)}}
$$

This function is called the **sigmoid** (or logistic) function. It maps any real value (from \( -\infty \) to \( +\infty \)) smoothly into the interval \( (0, 1) \).  
Thatâ€™s perfect for modeling probabilities.

---

#### Sigmoid function notation

Sometimes we write the sigmoid more generally as:

$$
S(y) = \frac{1}{1 + e^{-y}}
$$

Where \( y = mx + b \). This gives us the classic **S-shaped curve** of logistic regression.

---

#### ðŸ“Œ Real-Life Examples of Logistic Regression

1. **Email Spam Classification**  
   - Input: Frequency of spammy words, sender reputation  
   - Output: Probability that an email is spam (1 = spam, 0 = not spam)

2. **Loan Default Prediction**  
   - Input: Credit score, income, loan amount  
   - Output: Probability a person will default on their loan

3. **Medical Diagnosis**  
   - Input: Age, blood pressure, test results  
   - Output: Probability a patient has a disease (e.g., heart disease)

4. **Customer Churn**  
   - Input: Usage patterns, support tickets, subscription duration  
   - Output: Probability a customer will cancel the service

5. **Online Advertising**  
   - Input: User demographics, ad features, time of day  
   - Output: Probability that a user will click an ad

---

Logistic regression helps us answer **yes-or-no questions** based on input data â€” making it one of the most widely used classification tools in data science.



```{python}
import numpy as np
import matplotlib.pyplot as plt

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Generate values for x (input)
x = np.linspace(-10, 10, 400)

# Apply the sigmoid function
y = sigmoid(x)

# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(x, y, label=r'$\frac{1}{1 + e^{-x}}$', color='b', lw=2)

# Add labels and title
plt.title("Sigmoid Function", fontsize=16)
plt.xlabel("x", fontsize=14)
plt.ylabel("Sigmoid(x)", fontsize=14)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.grid(True)

# Show the plot
plt.legend()
plt.show()
```


```{python}
from sklearn.linear_model import LogisticRegression
```


```{python}
lg_model = LogisticRegression()
lg_model = lg_model.fit(x_train, y_train)
```


```{python}
y_pred = lg_model.predict(x_test)
print(classification_report(y_test, y_pred))
```


### ðŸš€ Support Vector Machines (SVM) â€“ An Intuitive Introduction

#### **What is a Support Vector Machine (SVM)?**

A **Support Vector Machine (SVM)** is a powerful algorithm used for both classification and regression tasks, but it is most commonly used for **classification** problems.

The goal of an SVM is to find the **best boundary (or hyperplane)** that separates data into different classes. This boundary is called the **decision boundary**. In simple terms, an SVM tries to create a line (in 2D) or a hyperplane (in higher dimensions) that best divides the data into two groups.

#### **How does SVM work?**

Letâ€™s break it down:

1. **Linearly Separable Data:**
   If the data is linearly separable (i.e., it can be separated with a straight line or hyperplane), the SVM finds the optimal line that divides the two classes.
   
   The "optimal" line is the one that maximizes the **margin** between the two classes, meaning the line is placed in such a way that the distance between the closest data points (called **support vectors**) of each class is as large as possible.

2. **Margin and Support Vectors:**
   The points that are closest to the decision boundary are called **support vectors**. These points are critical because they are used to determine the position of the decision boundary. The larger the margin between the support vectors of the two classes, the better the model.

3. **Non-Linearly Separable Data:**
   When the data is not linearly separable (i.e., cannot be divided with a straight line), the SVM uses a **kernel trick** to transform the data into a higher-dimensional space where a hyperplane can be used to separate the data. This allows SVM to work in more complex scenarios.

#### **Real-Life Examples of SVM:**

- **Image Classification**: SVM can be used to classify images, like identifying cats and dogs in photos.
- **Medical Diagnosis**: SVM is useful for classifying patients as healthy or diseased based on medical test results.
- **Spam Detection**: SVM can be used to classify emails as spam or not spam based on their content.
- **Face Recognition**: SVM helps to distinguish different people in facial recognition systems.

---

### **Visualizing a Simple SVM in 2D**

We will now visualize how SVM works for a simple 2D dataset. We'll create two classes and show the **decision boundary**, the **support vectors**, and the **margin**.

---


```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# Create a simple 2D dataset (corrected)
X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1)

# Split into train and test sets
X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an SVM classifier
svm = SVC(kernel='linear', C=1)
svm.fit(X_train_1, y_train_1)

# Get the decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Create the plot
plt.figure(figsize=(8, 6))

# Plot the decision boundary and margin
plt.contourf(xx, yy, Z, levels=[-1, 0, 1], cmap=plt.cm.coolwarm, alpha=0.2)
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')
plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], facecolors='yellow', edgecolors='black', s=100, label="Support Vectors")
plt.scatter(X_train_1[:, 0], X_train[:, 1], c=y_train_1, cmap=plt.cm.coolwarm, marker='o', label="Data Points")
plt.title("SVM with Linear Kernel", fontsize=16)
plt.xlabel("Feature 1", fontsize=14)
plt.ylabel("Feature 2", fontsize=14)
plt.legend()

# Show the plot
plt.show()
```



```{python}
from sklearn.svm import SVC
svm_model = SVC()
svm_model = svm_model.fit(x_train, y_train)
```


```{python}
y_pred = svm_model.predict(x_test)
print(classification_report(y_test, y_pred))
```











